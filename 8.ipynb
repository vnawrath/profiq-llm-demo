{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give me data!\n",
    "\n",
    "![retrieval](retrieval.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "gpt35 = ChatOpenAI(\n",
    "    temperature=0, callbacks=[StreamingStdOutCallbackHandler()], streaming=True\n",
    ")\n",
    "\n",
    "gpt4 = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"2305.10601.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(pages)\n",
    "\n",
    "faiss_index = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    gpt4,\n",
    "    faiss_index.as_retriever(),\n",
    "    condense_question_llm=gpt35,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Tree of Thoughts: Deliberate Problem Solving\n",
      "with Large Language Models\n",
      "Shunyu Yao\n",
      "Princeton UniversityDian Yu\n",
      "Google DeepMindJeffrey Zhao\n",
      "Google DeepMindIzhak Shafran\n",
      "Google DeepMind\n",
      "Thomas L. Grifﬁths\n",
      "Princeton UniversityYuan Cao\n",
      "Google DeepMindKarthik Narasimhan\n",
      "Princeton University\n",
      "Abstract\n",
      "Language models are increasingly being deployed for general problem solving\n",
      "across a wide range of tasks, but are still conﬁned to token-level, left-to-right\n",
      "decision-making processes during inference. This means they can fall short in\n",
      "tasks that require exploration, strategic lookahead, or where initial decisions play\n",
      "a pivotal role. To surmount these challenges, we introduce a new framework for\n",
      "language model inference, “Tree of Thoughts” (ToT), which generalizes over the\n",
      "popular “Chain of Thought” approach to prompting language models, and enables\n",
      "exploration over coherent units of text (“thoughts”) that serve as intermediate steps\n",
      "\n",
      "do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\n",
      "options – the kind of heuristic-guided search that seems characteristic of human problem-solving.\n",
      "To address these shortcomings, we introduce Tree of Thoughts (ToT) , a paradigm that allows LMs to\n",
      "explore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\n",
      "over a tree, where each node is a states= [x;z1\u0001\u0001\u0001i]representing a partial solution with the input and\n",
      "the sequence of thoughts so far. A speciﬁc instantiation of ToT involves answering four questions:\n",
      "1. How to decompose the intermediate process into thought steps; 2. How to generate potential\n",
      "thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\n",
      "1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\n",
      "ToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\n",
      "\n",
      "backtracks to make more global decisions.\n",
      "To design such a planning process, we return to the origins of artiﬁcial intelligence (and cognitive\n",
      "science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\n",
      "starting in the 1950s [ 18,19]. Newell and colleagues characterized problem solving [ 18] as search\n",
      "through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\n",
      "(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\n",
      "existing methods (detailed below) sample continuous language sequences for problem solving, ToT\n",
      "actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\n",
      "as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\n",
      "LM to self-evaluate the progress different intermediate thoughts make towards solving the problem\n",
      "\n",
      "Broader impact. ToT is a framework that empowers LMs to more autonomously and intelligently\n",
      "make decisions and solve problems. While current tasks are limited to reasoning and search problems,\n",
      "future applications involving interaction with external environments or humans could bring potential\n",
      "danger, e.g. facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability\n",
      "of model decisions and the opportunity for human alignment, as the resulting representations are\n",
      "readable, high-level language reasoning instead of implicit, low-level token values.\n",
      "Conclusion. The associative “System 1” of LMs can be beneﬁcially augmented by a “System 2”\n",
      "based on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts\n",
      "framework provides a way to translate classical insights about problem-solving into actionable\n",
      "methods for contemporary LMs. At the same time, LMs address a weakness of these classical\n",
      "Human: What is the main principle of tree of thought?\u001b[0m\n",
      "The main principle of the Tree of Thoughts (ToT) framework is to enable language models to explore multiple reasoning paths over coherent units of text called \"thoughts\" for problem-solving. ToT frames any problem as a search over a tree, where each node represents a partial solution with the input and the sequence of thoughts so far. This approach allows language models to incorporate planning, lookahead, and backtracking, which are characteristic of human problem-solving, and helps overcome the limitations of token-level, left-to-right decision-making processes in traditional language models.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example of a data powered agent\n",
    "chat_history = []\n",
    "query = \"What is the main principle of tree of thought?\"\n",
    "\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the main principle of tree of thought?\n",
      "Assistant: The main principle of the Tree of Thoughts (ToT) framework is to enable language models to explore multiple reasoning paths over coherent units of text called \"thoughts\" for problem-solving. ToT frames any problem as a search over a tree, where each node represents a partial solution with the input and the sequence of thoughts so far. This approach allows language models to incorporate planning, lookahead, and backtracking, which are characteristic of human problem-solving, and helps overcome the limitations of token-level, left-to-right decision-making processes in traditional language models.\n",
      "Follow Up Input: How is the backtracing implemented?\n",
      "Standalone question:\u001b[0m\n",
      "Can you explain how the backtracking is implemented in the Tree of Thoughts framework?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\n",
      "options – the kind of heuristic-guided search that seems characteristic of human problem-solving.\n",
      "To address these shortcomings, we introduce Tree of Thoughts (ToT) , a paradigm that allows LMs to\n",
      "explore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\n",
      "over a tree, where each node is a states= [x;z1\u0001\u0001\u0001i]representing a partial solution with the input and\n",
      "the sequence of thoughts so far. A speciﬁc instantiation of ToT involves answering four questions:\n",
      "1. How to decompose the intermediate process into thought steps; 2. How to generate potential\n",
      "thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\n",
      "1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\n",
      "ToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\n",
      "\n",
      "backtracks to make more global decisions.\n",
      "To design such a planning process, we return to the origins of artiﬁcial intelligence (and cognitive\n",
      "science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\n",
      "starting in the 1950s [ 18,19]. Newell and colleagues characterized problem solving [ 18] as search\n",
      "through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\n",
      "(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\n",
      "existing methods (detailed below) sample continuous language sequences for problem solving, ToT\n",
      "actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\n",
      "as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\n",
      "LM to self-evaluate the progress different intermediate thoughts make towards solving the problem\n",
      "\n",
      "Tree of Thoughts: Deliberate Problem Solving\n",
      "with Large Language Models\n",
      "Shunyu Yao\n",
      "Princeton UniversityDian Yu\n",
      "Google DeepMindJeffrey Zhao\n",
      "Google DeepMindIzhak Shafran\n",
      "Google DeepMind\n",
      "Thomas L. Grifﬁths\n",
      "Princeton UniversityYuan Cao\n",
      "Google DeepMindKarthik Narasimhan\n",
      "Princeton University\n",
      "Abstract\n",
      "Language models are increasingly being deployed for general problem solving\n",
      "across a wide range of tasks, but are still conﬁned to token-level, left-to-right\n",
      "decision-making processes during inference. This means they can fall short in\n",
      "tasks that require exploration, strategic lookahead, or where initial decisions play\n",
      "a pivotal role. To surmount these challenges, we introduce a new framework for\n",
      "language model inference, “Tree of Thoughts” (ToT), which generalizes over the\n",
      "popular “Chain of Thought” approach to prompting language models, and enables\n",
      "exploration over coherent units of text (“thoughts”) that serve as intermediate steps\n",
      "\n",
      "4. Search algorithm. Finally, within the ToT framework, one can plug and play different search\n",
      "algorithms depending on the tree structure. We explore two relatively simple search algorithms and\n",
      "leave more advanced ones (e.g. A* [9], MCTS [2]) for future work:\n",
      "(a)Breadth-ﬁrst search (BFS) (Algorithm 1) maintains a set of the bmost promising states\n",
      "per step. This is used for Game of 24 and Creative Writing where the tree depth is limit\n",
      "(T\u00143), and initial thought steps can be evaluated and pruned to a small set ( b\u00145).\n",
      "(b)Depth-ﬁrst search (DFS) (Algorithm 2) explores the most promising state ﬁrst, until the\n",
      "ﬁnal output is reached ( t > T ), or the state evaluator deems it impossible to solve the\n",
      "problem from the current s(V(p\u0012;fsg)(s)\u0014vthfor a value threshold vth). In the latter\n",
      "case, the subtree from sispruned to trade exploration for exploitation. In both cases, DFS\n",
      "backtracks to the parent state of sto continue exploration.\n",
      "Human: Can you explain how the backtracking is implemented in the Tree of Thoughts framework?\u001b[0m\n",
      "In the Tree of Thoughts (ToT) framework, backtracking is implemented through the use of search algorithms, specifically Depth-first search (DFS). DFS explores the most promising state first, going deeper into the tree until the final output is reached or the state evaluator deems it impossible to solve the problem from the current state. In the latter case, the subtree from the current state is pruned to trade exploration for exploitation. When either of these conditions is met, DFS backtracks to the parent state of the current state to continue exploration. This backtracking process allows the model to explore alternative reasoning paths and make more global decisions, which is essential for tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"How is the backtracing implemented?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
